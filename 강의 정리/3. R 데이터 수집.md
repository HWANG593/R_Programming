# R 데이터 수집

## R 패키지

* R을 가지고 할 수 있는 통계, 분석 그리고 시각화와 관련하여 기능을 정의한 함수들의 묶음
* R을 설치할 때 기본 패키지가 있고 추가로 설치할 수 있다.

```R
# 새로운 R 패키지의 설치
install.packages("패키지명")

# 이미 설치된 R 패키지 확인
installed.packages()

# 설치된 패키지 삭제
remove.packages("패키지명")

# 설치된 패키지의 버전 확인
packageVersion("패키지명")

# 설치된 패키지 업데이트
update.packages("패키지명")

# 설치된 패키지 로드
library(패키지명)
require(패키지명)

# 로드된 패키지 언로드(로드상태 해제)
detach("package:패키지명")

# 로드된 패키지 점검
search()
```

<br>

---

<br>

## 데이터 수집

### 웹 크롤링(web crawling)

* 자동화 봇(bot)인 웹 크롤러가 정해진 규칙에 따라 복수 개의 웹 페이지를 브라우징 하는 행위

<br>

### 웹 스크래핑(web scraping)

* 웹 사이트 상에서 원하는 부분에 위치한 정보를 컴퓨터로 자동으로 추출하여 수집하는 기술 

#### CSS Selector

* 스크래핑하려는 페이지에서 원하는 태그 찾기

![css선택자](https://github.com/HWANG593/R_Programming/blob/master/%EC%9D%B4%EB%AF%B8%EC%A7%80/css%EC%84%A0%ED%83%9D%EC%9E%90.JPG?raw=true)

<br>

#### Xpath

* 스크래핑하려는 페이지에서 원하는 태그 찾기 
* 태그가 없는 것에 대해 스크래핑할 경우 Xpath 필요

```R
# //의 경우 조상에 관계없이 라는 뜻

html_nodes(text, xpath = "//*[@id='old_content']/table/tbody/tr/td[2]/text()")
```

<br>

### 정적 웹 페이지 스크래핑에 사용되는 주요 API

#### xml2 패키지

* read_html(url) : HTML 웹 페이지를 요청해서 받아오기

<br>

#### rvest 패키지

* html_nodes(x, css, xpath) or html_node(x, css, xpath) : 원하는 노드(태그) 추출하기
* html_text(x, trim = FALSE) : 노드에서 컨텐트 추출하기
* html_attrs(x) : 노드에서 속성들 추출하기
* html_attr(x, name, default = "") : 노드에서 주어진 명칭의 속성 값 추출하기

<br>

#### XML 패키지

* htmlParse(file, encoding="...") : xpathSApply() 사용 가능한 객체로 변환 (doc으로 들어감)
* xpathSApply(doc, path, fun) : 원하는 노드(태그) 추출하고 전달된 함수 수행하기
* fun : xmlValue, xmlGetAttr, xmlAttrs

<br>

#### httr 패키지

* GET(url) : HTML 웹 페이지를 요청해서 받아오기
* 요청 헤더에 계정 또는 패스워드 등의 정보 전달 가능
* 응답 내용이 바이너리인 경우에도 사용 가능
* 바이너리란 텍스트 문서 (메모장으로 열었을 때 콘텐츠가 표시되는 것)가 아닌 것
* 바이너리란 전용 프로그램으로만 볼 수 있는 것

<br>

---

<br>

### Chrome 의 개발자 도구 활용

* 마우스 포인트로 HTML 태그 찾기
* 우클릭으로 Copy Selector, Copy Xpath 활용