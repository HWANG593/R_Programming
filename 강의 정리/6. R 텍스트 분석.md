# R 텍스트 분석

## 텍스트 마이닝

### 텍스트 마이닝

> 텍스트 마이닝은 단어의 출현 빈도, 단어 간 관계성 등을 파악하여 유의미한 정보를 추출하는 것
>
> 자연어 처리 기술을 기반으로 함

* 비정형 텍스트  >  구조화된 데이터  >  의미있는 정보 추출  >  연관 분석, 트렌드 분석, 감성 분석

<br>

### 자연어 처리

>일상 생활에서 사용하는 말, 언어
>
>자연어 처리 기술을 바탕으로 텍스트를 분석하거나 중요한 단어, 문장을 추출할 수 있음

* 형태소 분석으로 품사를 파악한 후 단어를 추출해 각 단어가 얼마나 등장했는지 확인

```R
install.packages("KoNLP")
library(KoNLP)

SimplePos22("동해물과 백두산이 마르고 닳도록")
extractNoun("동해물과 백두산이 마르고 닳도록")
sapply("동해물과 백두산이 마르고 닳도록", extractNoun, USE.NAMES=P)
```

<br>

### 텍스트 전처리와 tm 패키지

#### tm 패키지를 이용한 텍스트 천처리 (tm_map() 함수)

* tm 패키지는 텍스트 데이터의 정제작업을 지원하는 다양한 변환함수를 제공한다.
* 문서에서 문장 부호를 제거, 문자를 모두 소문자로 변환, 단어의 어간을 추출해주는 스테밍을 적용할 수 있다.

<br>

#### 코퍼스

> 텍스트 마이닝을 위해 수행해야 할 첫 번째 작업은 비정형의 텍스트를 구조화된 데이터로 변환하는 것이다.

* 코퍼스(corpus) : 텍스트 마이닝 패키지인 tm에서 문서를 관리하는 기본구조
* 텍스트 문서들의 집합을 의미
* 분석 대상이 되는 개별 텍스트 문서를 단어의 집합으로 단순화시킨 표현 방법
* 단어의 순서나 문법은 무시하고 단어의 출현 빈도만을 이용해 텍스트를 매트릭스로 표현한다.
* TDM(term-document-matrix) 또는 DTM(document-term-matrix)라고 한다.

```R
lunch <- c("커피 파스타 치킨 샐러드 아이스크림",
			"커피 우동 소고기김밥 귤",
			"참치김밥 커피 오뎅",
			"샐러드 피자 파스타 콜라",
			"티라무슈 햄버거 콜라",
			"파스타 샐러드 커피"
)

cps <- VCorpus(VectorSource(lunch))
tdm <- TermDocumentMatrix(cps)
(m <- as.matrix(tdm))

# 단어 수는 default = 3 이므로 옵션을 통해 변경
cps <- VCorpus(VectorSource(lunch))
tdm <- TermDocumentMatrix(cps, control=list(wordLengths = c(1, Inf)))
(m <- as.matrix(tdm))

rowSums(m)
colSums(m)

# 동시출현이란 한 문장, 문단 또는 텍스트 단위에서 같이 출현한 단어를 가리킨다.
com <- m %*% t(m) 
```

<br>

#### 단어 가중치 

* 문서에서 어떤 단어의 중요도를 평가하기 위해 사용되는 통계적인 수치

| 약어  | 의미                                    |
| ----- | --------------------------------------- |
| TF    | Term Frequency (단어 빈도)              |
| IDF   | Inverse Document Frequency (역문서빈도) |
| DF    | Document Frequency (문서빈도)           |
| TFIDF | TF X IDF                                |

```R
library(tm)
A <- c('포도 바나나 딸기 맥주 비빔밥 여행 낚시 떡볶이 분홍색 듀크 귤')
B <- c('사과 와인 스테이크 배 포도 여행 등산 짜장면 냉면 삼겹살 파란색 듀크 귤 귤')
C <- c('백숙 바나나 맥주 여행 피자 콜라 햄버거 비빔밥 파란색 듀크 귤')
D <- c('귤 와인 스테이크 배 포도 햄버거 등산 갈비 냉면 삼겹살 녹색 듀크')
data <- c(A,B,C,D)
cps <- Corpus(VectorSource(data))
tdm <- TermDocumentMatrix(cps)
inspect(tdm)
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing=T)

inspect(tdm) m <- as.matrix(tdm) v <- sort(rowSums(m), decreasing=T)
m1 <- as.matrix(weightTf(tdm))
m2. <- as.matrix(weightTfIdf(tdm))
```

* 특정 문서 내에서 단어 빈도가 높을 수록, 전체 문서들엔 그 단어를 포함한 문서가 적을 수록 TFIDF 값이 높아지게 된다.
* 즉, 문서 내에서 해당 단어의 중요도는 커지게 된다.

<br>

#### tm 패키지를 이용한 예제

```R
html.parsed <- htmlParse("TextofSteveJobs.html") 
text <- xpathSApply(html.parsed, path="//p", xmlValue) 

text <- text[4:30]  
docs <- VCorpus(VectorSource(text)) 

toSpace <- content_transformer(function(x, pattern){return(gsub(pattern, "", x))})
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, ";")
docs <- tm_map(docs, toSpace, "'")
docs[[17]]
docs[[19]]
docs[[17]]$content
docs[[19]]$content
docs <- tm_map(docs, removePunctuation)
text[17]
docs[[17]]$content

docs <- tm_map(docs, content_transformer(tolower))
docs[[17]]$content
docs <- tm_map(docs, removeNumbers)
docs[[17]]$content
docs <- tm_map(docs, removeWords, stopwords("english"))
docs[[17]]$content
docs <- tm_map(docs,
stripWhitespace)
docs[[17]]$content
docs <- tm_map(docs,
stemDocument)
docs[[17]]$content

tdm <- TermDocumentMatrix(docs)
tdm
inspect(tdm[50:60, 1:5])
termFreq <- rowSums(as.matrix(tdm))
head(termFreq)
termFreq[head(order(termFreq, decreasing=T))]
```

<br>

### 문서간 유사도 분석

* 문서들간에 동일한 단어 또는 비슷한 단어가 얼마나 공통으로 많이 사용되었나에 따라 문서간 유사도 분석을 할 수 있다.

<br>

#### 코사인 유사도(Cosine Similarity)

* 두 벡터 간의 코사인 각도를 이용하여 유사도를 측정
* 두 벡터의 값이 동일하면 1, 반대 방향이면 -1, 90도의 각을 이루면 0 이된다.
* 1에 가까울수록 유사도가 높다.

<br>

#### 유클리디안 거리(Euclidean distance)

* 두 점 사이의 유클리드 거리 공식은 두 점 사이의 거리를 구하는 것과 동일

<br>

### 텍스트 마이닝의 결과 시각화

#### 워드 클라우드(Word Cloud)

```R
install.packages("wordcloud") 
library(wordcloud) 

wordcloud(words, freq, scale=c(4,.5), min.freq=3, max.words=Inf, 		random.order=TRUE, random.color=FALSE, rot.per=.1, colors="black",
ordered.colors=FALSE, use.r.layout=FALSE, fixed.asp=TRUE, ...)
```

* scale : 빈도가 가장 큰 단어와 가장 빈도가 작은 단어 폰트 사이 크기, scale=c(5,0.2.) 
* rot.per=0.1 : 90도 회전해서 보여줄 단어 비율 
* min.freq=3, max.words=100 : 빈도 3이상, 100미만 단어 표현 
* random.order=F : True(랜덤배치) / False(빈도수가 큰단어를 중앙에 배치) 
* random.color=T : True(색상랜덤) / False(빈도수순으로 색상표현) 
* colors=색상이름 
* family : 폰트 
* savePlot(szWordCloudImageFile, type="png") : WordCloud 결과를 이미지 파일로 저장

<br>

####  동시출현(Co - occurrence)

```R
install.packages("qgraph") 
library(qgraph) 

qgraph(com, labels=rownames(com), diag=F, layout='spring', edge.color='blue', vsize=log(diag(com)*800))
```

<br>

#### 바 그래프(단어 출현 횟수)

```R
barplot(termFreq[termFreq >= 7],horiz=T, 
		las=1, cex.names=0.8,col=rainbow(16), 
		xlab="word Frequency", ylab="Words")
```

 